{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tifffile import TiffFile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vgg16_bn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../platelet_data'\n",
    "\n",
    "with TiffFile(os.path.join(data_dir, 'train-images.tif')) as tif:\n",
    "    train_img = tif.asarray()\n",
    "    \n",
    "with TiffFile(os.path.join(data_dir, 'train-labels.tif')) as tif:\n",
    "    train_label = tif.asarray()\n",
    "    \n",
    "with TiffFile(os.path.join(data_dir, 'eval-images.tif')) as tif:\n",
    "    eval_img = tif.asarray()\n",
    "    \n",
    "with TiffFile(os.path.join(data_dir, 'eval-labels.tif')) as tif:\n",
    "    eval_label = tif.asarray()\n",
    "    \n",
    "with TiffFile(os.path.join(data_dir, 'test-images.tif')) as tif:\n",
    "    test_img = tif.asarray()\n",
    "    \n",
    "with TiffFile(os.path.join(data_dir, 'test-labels.tif')) as tif:\n",
    "    test_label = tif.asarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = max(np.max(train_img), np.max(eval_img), np.max(test_img))\n",
    "m = min(np.min(train_img), np.min(eval_img), np.min(test_img))\n",
    "train_img_norm = (train_img - m) / (M - m)\n",
    "eval_img_norm = (eval_img - m) / (M - m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 800)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 1, 1, 1],\n",
       "       [0, 0, 0, ..., 1, 1, 1],\n",
       "       [0, 0, 0, ..., 1, 1, 1],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 1, 1, 1],\n",
       "       [0, 0, 0, ..., 1, 1, 1],\n",
       "       [0, 0, 0, ..., 1, 1, 1]], dtype=uint16)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = train_label[0]\n",
    "print(label.shape)\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 800])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1]], dtype=torch.int16)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = torch.from_numpy(label.astype(np.int16))\n",
    "print(label.shape)\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 800])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = label.long()\n",
    "print(label.shape)\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([800, 800, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0, 0,  ..., 0, 0, 0],\n",
       "         [1, 0, 0,  ..., 0, 0, 0],\n",
       "         [1, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 1, 0,  ..., 0, 0, 0],\n",
       "         [0, 1, 0,  ..., 0, 0, 0],\n",
       "         [0, 1, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[1, 0, 0,  ..., 0, 0, 0],\n",
       "         [1, 0, 0,  ..., 0, 0, 0],\n",
       "         [1, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 1, 0,  ..., 0, 0, 0],\n",
       "         [0, 1, 0,  ..., 0, 0, 0],\n",
       "         [0, 1, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[1, 0, 0,  ..., 0, 0, 0],\n",
       "         [1, 0, 0,  ..., 0, 0, 0],\n",
       "         [1, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 1, 0,  ..., 0, 0, 0],\n",
       "         [0, 1, 0,  ..., 0, 0, 0],\n",
       "         [0, 1, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1, 0, 0,  ..., 0, 0, 0],\n",
       "         [1, 0, 0,  ..., 0, 0, 0],\n",
       "         [1, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 1, 0,  ..., 0, 0, 0],\n",
       "         [0, 1, 0,  ..., 0, 0, 0],\n",
       "         [0, 1, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[1, 0, 0,  ..., 0, 0, 0],\n",
       "         [1, 0, 0,  ..., 0, 0, 0],\n",
       "         [1, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 1, 0,  ..., 0, 0, 0],\n",
       "         [0, 1, 0,  ..., 0, 0, 0],\n",
       "         [0, 1, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[1, 0, 0,  ..., 0, 0, 0],\n",
       "         [1, 0, 0,  ..., 0, 0, 0],\n",
       "         [1, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 1, 0,  ..., 0, 0, 0],\n",
       "         [0, 1, 0,  ..., 0, 0, 0],\n",
       "         [0, 1, 0,  ..., 0, 0, 0]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = torch.nn.functional.one_hot(label, num_classes=7)\n",
    "print(label.shape)\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 800, 800])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = label.to(torch.float32)\n",
    "label = label.permute(2, 0, 1)\n",
    "print(label.shape)\n",
    "label[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 800)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.29224105, 0.28794763, 0.27185214, ..., 0.16621101, 0.15968425,\n",
       "        0.18733154],\n",
       "       [0.29041201, 0.2864459 , 0.28919908, ..., 0.14849827, 0.14524451,\n",
       "        0.15616095],\n",
       "       [0.29035425, 0.28727378, 0.27456681, ..., 0.17294956, 0.17814786,\n",
       "        0.17216018],\n",
       "       ...,\n",
       "       [0.21216789, 0.22475934, 0.21274548, ..., 0.10785522, 0.12606854,\n",
       "        0.12757027],\n",
       "       [0.17462457, 0.18203697, 0.19162495, ..., 0.11160955, 0.1406238 ,\n",
       "        0.14597613],\n",
       "       [0.19928764, 0.18492491, 0.16105121, ..., 0.12724297, 0.11033885,\n",
       "        0.10375433]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = train_img_norm[0]\n",
    "print(image.shape)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([800, 800, 1])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms.ToTensor()(image).shape\n",
    "transforms.ToTensor()(image)\n",
    "transforms.ToTensor()(image).permute(1, 2, 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        label = torch.from_numpy(label.astype(np.int64)).clone()\n",
    "        label = torch.nn.functional.one_hot(label.long(), num_classes=7)\n",
    "        label = label.to(torch.float32)\n",
    "        label = label.permute(2, 0, 1)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Transformations (if needed)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # Convert numpy array to PyTorch Tensor\n",
    "    # Add additional transformations here if required\n",
    "])\n",
    "\n",
    "\n",
    "# Creating the datasets\n",
    "train_dataset = CustomDataset(train_img_norm, train_label, transform=transform)\n",
    "eval_dataset = CustomDataset(eval_img_norm, eval_label, transform=transform)\n",
    "\n",
    "# DataLoader creation\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2792, 0.2540, 0.2778,  ..., 0.0720, 0.0688, 0.0777],\n",
       "         [0.2843, 0.2874, 0.2796,  ..., 0.0576, 0.0887, 0.0880],\n",
       "         [0.2452, 0.2731, 0.2922,  ..., 0.0762, 0.0782, 0.1064],\n",
       "         ...,\n",
       "         [0.1566, 0.1436, 0.1157,  ..., 0.0972, 0.1052, 0.1111],\n",
       "         [0.1813, 0.1771, 0.1658,  ..., 0.1242, 0.1452, 0.1126],\n",
       "         [0.1536, 0.1423, 0.1382,  ..., 0.1231, 0.1394, 0.1448]]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[10][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, middle_channels, kernel_size = 3, padding=\"same\")\n",
    "        self.bn1 = nn.BatchNorm2d(middle_channels)\n",
    "        self.rl = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(middle_channels, out_channels, kernel_size = 3, padding=\"same\")\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.rl(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.rl(x)\n",
    "        return x\n",
    "\n",
    "class UpConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size = 2, padding=\"same\")\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn2(x)\n",
    "        return x\n",
    "\n",
    "class UNet_2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.TCB1 = TwoConvBlock(1, 64, 64)\n",
    "        self.TCB2 = TwoConvBlock(64, 128, 128)\n",
    "        self.TCB3 = TwoConvBlock(128, 256, 256)\n",
    "        self.TCB4 = TwoConvBlock(256, 512, 512)\n",
    "        self.TCB5 = TwoConvBlock(512, 1024, 1024)\n",
    "        self.TCB6 = TwoConvBlock(1024, 512, 512)\n",
    "        self.TCB7 = TwoConvBlock(512, 256, 256)\n",
    "        self.TCB8 = TwoConvBlock(256, 128, 128)\n",
    "        self.TCB9 = TwoConvBlock(128, 64, 64)\n",
    "        self.maxpool = nn.MaxPool2d(2, stride = 2)\n",
    "        \n",
    "        self.UC1 = UpConv(1024, 512) \n",
    "        self.UC2 = UpConv(512, 256) \n",
    "        self.UC3 = UpConv(256, 128) \n",
    "        self.UC4= UpConv(128, 64)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(64, 7, kernel_size = 1)\n",
    "        self.soft = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.TCB1(x)\n",
    "        x1 = x\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.TCB2(x)\n",
    "        x2 = x\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.TCB3(x)\n",
    "        x3 = x\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.TCB4(x)\n",
    "        x4 = x\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.TCB5(x)\n",
    "\n",
    "        x = self.UC1(x)\n",
    "        x = torch.cat([x4, x], dim = 1)\n",
    "        x = self.TCB6(x)\n",
    "\n",
    "        x = self.UC2(x)\n",
    "        x = torch.cat([x3, x], dim = 1)\n",
    "        x = self.TCB7(x)\n",
    "\n",
    "        x = self.UC3(x)\n",
    "        x = torch.cat([x2, x], dim = 1)\n",
    "        x = self.TCB8(x)\n",
    "\n",
    "        x = self.UC4(x)\n",
    "        x = torch.cat([x1, x], dim = 1)\n",
    "        x = self.TCB9(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "unet = UNet_2D().to(device)\n",
    "optimizer = optim.Adam(unet.parameters(), lr=0.001)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 800, 800]             640\n",
      "       BatchNorm2d-2         [-1, 64, 800, 800]             128\n",
      "              ReLU-3         [-1, 64, 800, 800]               0\n",
      "            Conv2d-4         [-1, 64, 800, 800]          36,928\n",
      "       BatchNorm2d-5         [-1, 64, 800, 800]             128\n",
      "              ReLU-6         [-1, 64, 800, 800]               0\n",
      "      TwoConvBlock-7         [-1, 64, 800, 800]               0\n",
      "         MaxPool2d-8         [-1, 64, 400, 400]               0\n",
      "            Conv2d-9        [-1, 128, 400, 400]          73,856\n",
      "      BatchNorm2d-10        [-1, 128, 400, 400]             256\n",
      "             ReLU-11        [-1, 128, 400, 400]               0\n",
      "           Conv2d-12        [-1, 128, 400, 400]         147,584\n",
      "      BatchNorm2d-13        [-1, 128, 400, 400]             256\n",
      "             ReLU-14        [-1, 128, 400, 400]               0\n",
      "     TwoConvBlock-15        [-1, 128, 400, 400]               0\n",
      "        MaxPool2d-16        [-1, 128, 200, 200]               0\n",
      "           Conv2d-17        [-1, 256, 200, 200]         295,168\n",
      "      BatchNorm2d-18        [-1, 256, 200, 200]             512\n",
      "             ReLU-19        [-1, 256, 200, 200]               0\n",
      "           Conv2d-20        [-1, 256, 200, 200]         590,080\n",
      "      BatchNorm2d-21        [-1, 256, 200, 200]             512\n",
      "             ReLU-22        [-1, 256, 200, 200]               0\n",
      "     TwoConvBlock-23        [-1, 256, 200, 200]               0\n",
      "        MaxPool2d-24        [-1, 256, 100, 100]               0\n",
      "           Conv2d-25        [-1, 512, 100, 100]       1,180,160\n",
      "      BatchNorm2d-26        [-1, 512, 100, 100]           1,024\n",
      "             ReLU-27        [-1, 512, 100, 100]               0\n",
      "           Conv2d-28        [-1, 512, 100, 100]       2,359,808\n",
      "      BatchNorm2d-29        [-1, 512, 100, 100]           1,024\n",
      "             ReLU-30        [-1, 512, 100, 100]               0\n",
      "     TwoConvBlock-31        [-1, 512, 100, 100]               0\n",
      "        MaxPool2d-32          [-1, 512, 50, 50]               0\n",
      "           Conv2d-33         [-1, 1024, 50, 50]       4,719,616\n",
      "      BatchNorm2d-34         [-1, 1024, 50, 50]           2,048\n",
      "             ReLU-35         [-1, 1024, 50, 50]               0\n",
      "           Conv2d-36         [-1, 1024, 50, 50]       9,438,208\n",
      "      BatchNorm2d-37         [-1, 1024, 50, 50]           2,048\n",
      "             ReLU-38         [-1, 1024, 50, 50]               0\n",
      "     TwoConvBlock-39         [-1, 1024, 50, 50]               0\n",
      "         Upsample-40       [-1, 1024, 100, 100]               0\n",
      "      BatchNorm2d-41       [-1, 1024, 100, 100]           2,048\n",
      "           Conv2d-42        [-1, 512, 100, 100]       2,097,664\n",
      "      BatchNorm2d-43        [-1, 512, 100, 100]           1,024\n",
      "           UpConv-44        [-1, 512, 100, 100]               0\n",
      "           Conv2d-45        [-1, 512, 100, 100]       4,719,104\n",
      "      BatchNorm2d-46        [-1, 512, 100, 100]           1,024\n",
      "             ReLU-47        [-1, 512, 100, 100]               0\n",
      "           Conv2d-48        [-1, 512, 100, 100]       2,359,808\n",
      "      BatchNorm2d-49        [-1, 512, 100, 100]           1,024\n",
      "             ReLU-50        [-1, 512, 100, 100]               0\n",
      "     TwoConvBlock-51        [-1, 512, 100, 100]               0\n",
      "         Upsample-52        [-1, 512, 200, 200]               0\n",
      "      BatchNorm2d-53        [-1, 512, 200, 200]           1,024\n",
      "           Conv2d-54        [-1, 256, 200, 200]         524,544\n",
      "      BatchNorm2d-55        [-1, 256, 200, 200]             512\n",
      "           UpConv-56        [-1, 256, 200, 200]               0\n",
      "           Conv2d-57        [-1, 256, 200, 200]       1,179,904\n",
      "      BatchNorm2d-58        [-1, 256, 200, 200]             512\n",
      "             ReLU-59        [-1, 256, 200, 200]               0\n",
      "           Conv2d-60        [-1, 256, 200, 200]         590,080\n",
      "      BatchNorm2d-61        [-1, 256, 200, 200]             512\n",
      "             ReLU-62        [-1, 256, 200, 200]               0\n",
      "     TwoConvBlock-63        [-1, 256, 200, 200]               0\n",
      "         Upsample-64        [-1, 256, 400, 400]               0\n",
      "      BatchNorm2d-65        [-1, 256, 400, 400]             512\n",
      "           Conv2d-66        [-1, 128, 400, 400]         131,200\n",
      "      BatchNorm2d-67        [-1, 128, 400, 400]             256\n",
      "           UpConv-68        [-1, 128, 400, 400]               0\n",
      "           Conv2d-69        [-1, 128, 400, 400]         295,040\n",
      "      BatchNorm2d-70        [-1, 128, 400, 400]             256\n",
      "             ReLU-71        [-1, 128, 400, 400]               0\n",
      "           Conv2d-72        [-1, 128, 400, 400]         147,584\n",
      "      BatchNorm2d-73        [-1, 128, 400, 400]             256\n",
      "             ReLU-74        [-1, 128, 400, 400]               0\n",
      "     TwoConvBlock-75        [-1, 128, 400, 400]               0\n",
      "         Upsample-76        [-1, 128, 800, 800]               0\n",
      "      BatchNorm2d-77        [-1, 128, 800, 800]             256\n",
      "           Conv2d-78         [-1, 64, 800, 800]          32,832\n",
      "      BatchNorm2d-79         [-1, 64, 800, 800]             128\n",
      "           UpConv-80         [-1, 64, 800, 800]               0\n",
      "           Conv2d-81         [-1, 64, 800, 800]          73,792\n",
      "      BatchNorm2d-82         [-1, 64, 800, 800]             128\n",
      "             ReLU-83         [-1, 64, 800, 800]               0\n",
      "           Conv2d-84         [-1, 64, 800, 800]          36,928\n",
      "      BatchNorm2d-85         [-1, 64, 800, 800]             128\n",
      "             ReLU-86         [-1, 64, 800, 800]               0\n",
      "     TwoConvBlock-87         [-1, 64, 800, 800]               0\n",
      "           Conv2d-88          [-1, 7, 800, 800]             455\n",
      "================================================================\n",
      "Total params: 31,048,519\n",
      "Trainable params: 31,048,519\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 2.44\n",
      "Forward/backward pass size (MB): 12622.07\n",
      "Params size (MB): 118.44\n",
      "Estimated Total Size (MB): 12742.95\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(unet, (1, 800, 800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(output, target):\n",
    "    return nn.BCEWithLogitsLoss()(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x16c827b80>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 800, 800])\n",
      "torch.Size([4, 7, 800, 800])\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imyu/anaconda3/envs/seg/lib/python3.9/site-packages/torch/nn/modules/conv.py:456: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Convolution.cpp:1009.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 14.07 GB, other allocations: 4.14 GB, max allowed: 18.13 GB). Tried to allocate 1.22 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/imyu/transx/kikagaku_seg/src/02.ipynb セル 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/02.ipynb#X16sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mprint\u001b[39m(inputs\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/02.ipynb#X16sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/02.ipynb#X16sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m outputs \u001b[39m=\u001b[39m unet(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/02.ipynb#X16sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/02.ipynb#X16sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/seg/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/seg/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/imyu/transx/kikagaku_seg/src/02.ipynb セル 17\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/02.ipynb#X16sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([x2, x], dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/02.ipynb#X16sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mTCB8(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/02.ipynb#X16sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mUC4(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/02.ipynb#X16sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([x1, x], dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/02.ipynb#X16sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mTCB9(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/seg/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/seg/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/imyu/transx/kikagaku_seg/src/02.ipynb セル 17\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/02.ipynb#X16sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/02.ipynb#X16sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/02.ipynb#X16sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/02.ipynb#X16sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/02.ipynb#X16sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/seg/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/seg/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/seg/lib/python3.9/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/seg/lib/python3.9/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 14.07 GB, other allocations: 4.14 GB, max allowed: 18.13 GB). Tried to allocate 1.22 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "history = {\"train_loss\": []}\n",
    "n = 0\n",
    "m = 0\n",
    "\n",
    "for epoch in range(15):\n",
    "  train_loss = 0\n",
    "  val_loss = 0\n",
    "\n",
    "  unet.train()\n",
    "  for i, data in enumerate(train_loader):\n",
    "    inputs, labels = data\n",
    "    print(inputs.shape)\n",
    "    print(labels.shape)\n",
    "    inputs = inputs.float().to(device)\n",
    "    labels = labels.float().to(device)\n",
    "    print(inputs.dtype)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = unet(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()\n",
    "    history[\"train_loss\"].append(loss.item())\n",
    "    n += 1\n",
    "    # if i % ((len(df)//BATCH_SIZE)//10) == (len(df)//BATCH_SIZE)//10 - 1:\n",
    "    print(f\"epoch:{epoch+1}  index:{i+1}  train_loss:{train_loss/n:.5f}\")\n",
    "    n = 0\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "\n",
    "\n",
    "  unet.eval()\n",
    "  with torch.no_grad():\n",
    "    for i, data in enumerate(eval_loader):\n",
    "      inputs, labels = data\n",
    "      inputs = inputs.float().to(device)\n",
    "      labels = labels.float().to(device)\n",
    "      outputs = unet(inputs)\n",
    "      loss = criterion(outputs, labels)\n",
    "      val_loss += loss.item()\n",
    "      m += 1\n",
    "      # if i % (len(val_df)//BATCH_SIZE) == len(val_df)//BATCH_SIZE - 1:\n",
    "      print(f\"epoch:{epoch+1}  index:{i+1}  val_loss:{val_loss/m:.5f}\")\n",
    "      m = 0\n",
    "      val_loss = 0\n",
    "      val_acc = 0\n",
    "\n",
    "  # torch.save(unet.state_dict(), f\"./train_{epoch+1}.pth\")\n",
    "print(\"finish training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet_2D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.TCB1 = TwoConvBlock(1, 32, 32)\n",
    "        self.TCB2 = TwoConvBlock(32, 64, 64)\n",
    "        self.TCB3 = TwoConvBlock(64, 128, 128)\n",
    "        self.TCB4 = TwoConvBlock(128, 256, 256)\n",
    "        self.TCB5 = TwoConvBlock(256, 512, 512)\n",
    "        self.TCB6 = TwoConvBlock(512, 256, 256)\n",
    "        self.TCB7 = TwoConvBlock(256, 128, 128)\n",
    "        self.TCB8 = TwoConvBlock(128, 64, 64)\n",
    "        self.TCB9 = TwoConvBlock(64, 32, 32)\n",
    "        self.maxpool = nn.MaxPool2d(2, stride = 2)\n",
    "        \n",
    "        self.UC1 = UpConv(512, 256) \n",
    "        self.UC2 = UpConv(256, 128) \n",
    "        self.UC3 = UpConv(128, 64) \n",
    "        self.UC4= UpConv(64, 32)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(32, 7, kernel_size = 1)\n",
    "        self.soft = nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.TCB1(x)\n",
    "        x1 = x\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.TCB2(x)\n",
    "        x2 = x\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.TCB3(x)\n",
    "        x3 = x\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.TCB4(x)\n",
    "        x4 = x\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.TCB5(x)\n",
    "\n",
    "        x = self.UC1(x)\n",
    "        x = torch.cat([x4, x], dim = 1)\n",
    "        x = self.TCB6(x)\n",
    "\n",
    "        x = self.UC2(x)\n",
    "        x = torch.cat([x3, x], dim = 1)\n",
    "        x = self.TCB7(x)\n",
    "\n",
    "        x = self.UC3(x)\n",
    "        x = torch.cat([x2, x], dim = 1)\n",
    "        x = self.TCB8(x)\n",
    "\n",
    "        x = self.UC4(x)\n",
    "        x = torch.cat([x1, x], dim = 1)\n",
    "        x = self.TCB9(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "unet = UNet_2D().to(device)\n",
    "optimizer = optim.Adam(unet.parameters(), lr=0.001)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 800, 800]             320\n",
      "       BatchNorm2d-2         [-1, 32, 800, 800]              64\n",
      "              ReLU-3         [-1, 32, 800, 800]               0\n",
      "            Conv2d-4         [-1, 32, 800, 800]           9,248\n",
      "       BatchNorm2d-5         [-1, 32, 800, 800]              64\n",
      "              ReLU-6         [-1, 32, 800, 800]               0\n",
      "      TwoConvBlock-7         [-1, 32, 800, 800]               0\n",
      "         MaxPool2d-8         [-1, 32, 400, 400]               0\n",
      "            Conv2d-9         [-1, 64, 400, 400]          18,496\n",
      "      BatchNorm2d-10         [-1, 64, 400, 400]             128\n",
      "             ReLU-11         [-1, 64, 400, 400]               0\n",
      "           Conv2d-12         [-1, 64, 400, 400]          36,928\n",
      "      BatchNorm2d-13         [-1, 64, 400, 400]             128\n",
      "             ReLU-14         [-1, 64, 400, 400]               0\n",
      "     TwoConvBlock-15         [-1, 64, 400, 400]               0\n",
      "        MaxPool2d-16         [-1, 64, 200, 200]               0\n",
      "           Conv2d-17        [-1, 128, 200, 200]          73,856\n",
      "      BatchNorm2d-18        [-1, 128, 200, 200]             256\n",
      "             ReLU-19        [-1, 128, 200, 200]               0\n",
      "           Conv2d-20        [-1, 128, 200, 200]         147,584\n",
      "      BatchNorm2d-21        [-1, 128, 200, 200]             256\n",
      "             ReLU-22        [-1, 128, 200, 200]               0\n",
      "     TwoConvBlock-23        [-1, 128, 200, 200]               0\n",
      "        MaxPool2d-24        [-1, 128, 100, 100]               0\n",
      "           Conv2d-25        [-1, 256, 100, 100]         295,168\n",
      "      BatchNorm2d-26        [-1, 256, 100, 100]             512\n",
      "             ReLU-27        [-1, 256, 100, 100]               0\n",
      "           Conv2d-28        [-1, 256, 100, 100]         590,080\n",
      "      BatchNorm2d-29        [-1, 256, 100, 100]             512\n",
      "             ReLU-30        [-1, 256, 100, 100]               0\n",
      "     TwoConvBlock-31        [-1, 256, 100, 100]               0\n",
      "        MaxPool2d-32          [-1, 256, 50, 50]               0\n",
      "           Conv2d-33          [-1, 512, 50, 50]       1,180,160\n",
      "      BatchNorm2d-34          [-1, 512, 50, 50]           1,024\n",
      "             ReLU-35          [-1, 512, 50, 50]               0\n",
      "           Conv2d-36          [-1, 512, 50, 50]       2,359,808\n",
      "      BatchNorm2d-37          [-1, 512, 50, 50]           1,024\n",
      "             ReLU-38          [-1, 512, 50, 50]               0\n",
      "     TwoConvBlock-39          [-1, 512, 50, 50]               0\n",
      "         Upsample-40        [-1, 512, 100, 100]               0\n",
      "      BatchNorm2d-41        [-1, 512, 100, 100]           1,024\n",
      "           Conv2d-42        [-1, 256, 100, 100]         524,544\n",
      "      BatchNorm2d-43        [-1, 256, 100, 100]             512\n",
      "           UpConv-44        [-1, 256, 100, 100]               0\n",
      "           Conv2d-45        [-1, 256, 100, 100]       1,179,904\n",
      "      BatchNorm2d-46        [-1, 256, 100, 100]             512\n",
      "             ReLU-47        [-1, 256, 100, 100]               0\n",
      "           Conv2d-48        [-1, 256, 100, 100]         590,080\n",
      "      BatchNorm2d-49        [-1, 256, 100, 100]             512\n",
      "             ReLU-50        [-1, 256, 100, 100]               0\n",
      "     TwoConvBlock-51        [-1, 256, 100, 100]               0\n",
      "         Upsample-52        [-1, 256, 200, 200]               0\n",
      "      BatchNorm2d-53        [-1, 256, 200, 200]             512\n",
      "           Conv2d-54        [-1, 128, 200, 200]         131,200\n",
      "      BatchNorm2d-55        [-1, 128, 200, 200]             256\n",
      "           UpConv-56        [-1, 128, 200, 200]               0\n",
      "           Conv2d-57        [-1, 128, 200, 200]         295,040\n",
      "      BatchNorm2d-58        [-1, 128, 200, 200]             256\n",
      "             ReLU-59        [-1, 128, 200, 200]               0\n",
      "           Conv2d-60        [-1, 128, 200, 200]         147,584\n",
      "      BatchNorm2d-61        [-1, 128, 200, 200]             256\n",
      "             ReLU-62        [-1, 128, 200, 200]               0\n",
      "     TwoConvBlock-63        [-1, 128, 200, 200]               0\n",
      "         Upsample-64        [-1, 128, 400, 400]               0\n",
      "      BatchNorm2d-65        [-1, 128, 400, 400]             256\n",
      "           Conv2d-66         [-1, 64, 400, 400]          32,832\n",
      "      BatchNorm2d-67         [-1, 64, 400, 400]             128\n",
      "           UpConv-68         [-1, 64, 400, 400]               0\n",
      "           Conv2d-69         [-1, 64, 400, 400]          73,792\n",
      "      BatchNorm2d-70         [-1, 64, 400, 400]             128\n",
      "             ReLU-71         [-1, 64, 400, 400]               0\n",
      "           Conv2d-72         [-1, 64, 400, 400]          36,928\n",
      "      BatchNorm2d-73         [-1, 64, 400, 400]             128\n",
      "             ReLU-74         [-1, 64, 400, 400]               0\n",
      "     TwoConvBlock-75         [-1, 64, 400, 400]               0\n",
      "         Upsample-76         [-1, 64, 800, 800]               0\n",
      "      BatchNorm2d-77         [-1, 64, 800, 800]             128\n",
      "           Conv2d-78         [-1, 32, 800, 800]           8,224\n",
      "      BatchNorm2d-79         [-1, 32, 800, 800]              64\n",
      "           UpConv-80         [-1, 32, 800, 800]               0\n",
      "           Conv2d-81         [-1, 32, 800, 800]          18,464\n",
      "      BatchNorm2d-82         [-1, 32, 800, 800]              64\n",
      "             ReLU-83         [-1, 32, 800, 800]               0\n",
      "           Conv2d-84         [-1, 32, 800, 800]           9,248\n",
      "      BatchNorm2d-85         [-1, 32, 800, 800]              64\n",
      "             ReLU-86         [-1, 32, 800, 800]               0\n",
      "     TwoConvBlock-87         [-1, 32, 800, 800]               0\n",
      "           Conv2d-88          [-1, 7, 800, 800]             231\n",
      "================================================================\n",
      "Total params: 7,768,487\n",
      "Trainable params: 7,768,487\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 2.44\n",
      "Forward/backward pass size (MB): 6328.12\n",
      "Params size (MB): 29.63\n",
      "Estimated Total Size (MB): 6360.20\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(unet, (1, 800, 800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "unet = UNet_2D().to(device)\n",
    "optimizer = optim.Adam(unet.parameters(), lr=0.001)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 800, 800])\n",
      "torch.Size([4, 7, 800, 800])\n",
      "torch.float32\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 17.02 GB, other allocations: 1.12 GB, max allowed: 18.13 GB). Tried to allocate 1024 bytes on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/imyu/transx/kikagaku_seg/src/02.ipynb セル 23\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/02.ipynb#X32sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mprint\u001b[39m(inputs\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/02.ipynb#X32sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/02.ipynb#X32sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m outputs \u001b[39m=\u001b[39m unet(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/02.ipynb#X32sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/02.ipynb#X32sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/seg/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/seg/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/imyu/transx/kikagaku_seg/src/02.ipynb セル 23\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/02.ipynb#X32sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mUC1(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/02.ipynb#X32sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([x4, x], dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/02.ipynb#X32sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mTCB6(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/02.ipynb#X32sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mUC2(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/02.ipynb#X32sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([x3, x], dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/seg/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/seg/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/imyu/transx/kikagaku_seg/src/02.ipynb セル 23\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/02.ipynb#X32sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/02.ipynb#X32sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/02.ipynb#X32sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn1(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/02.ipynb#X32sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrl(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/02.ipynb#X32sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/seg/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/seg/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/seg/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/seg/lib/python3.9/site-packages/torch/nn/functional.py:2478\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2476\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2478\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   2479\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2480\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 17.02 GB, other allocations: 1.12 GB, max allowed: 18.13 GB). Tried to allocate 1024 bytes on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "history = {\"train_loss\": []}\n",
    "n = 0\n",
    "m = 0\n",
    "\n",
    "for epoch in range(15):\n",
    "  train_loss = 0\n",
    "  val_loss = 0\n",
    "\n",
    "  unet.train()\n",
    "  for i, data in enumerate(train_loader):\n",
    "    inputs, labels = data\n",
    "    print(inputs.shape)\n",
    "    print(labels.shape)\n",
    "    inputs = inputs.float().to(device)\n",
    "    labels = labels.float().to(device)\n",
    "    print(inputs.dtype)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = unet(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()\n",
    "    history[\"train_loss\"].append(loss.item())\n",
    "    n += 1\n",
    "    # if i % ((len(df)//BATCH_SIZE)//10) == (len(df)//BATCH_SIZE)//10 - 1:\n",
    "    print(f\"epoch:{epoch+1}  index:{i+1}  train_loss:{train_loss/n:.5f}\")\n",
    "    n = 0\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "\n",
    "\n",
    "  unet.eval()\n",
    "  with torch.no_grad():\n",
    "    for i, data in enumerate(eval_loader):\n",
    "      inputs, labels = data\n",
    "      inputs = inputs.float().to(device)\n",
    "      labels = labels.float().to(device)\n",
    "      outputs = unet(inputs)\n",
    "      loss = criterion(outputs, labels)\n",
    "      val_loss += loss.item()\n",
    "      m += 1\n",
    "      # if i % (len(val_df)//BATCH_SIZE) == len(val_df)//BATCH_SIZE - 1:\n",
    "      print(f\"epoch:{epoch+1}  index:{i+1}  val_loss:{val_loss/m:.5f}\")\n",
    "      m = 0\n",
    "      val_loss = 0\n",
    "      val_acc = 0\n",
    "\n",
    "  # torch.save(unet.state_dict(), f\"./train_{epoch+1}.pth\")\n",
    "print(\"finish training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
