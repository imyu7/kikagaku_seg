{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## segmentation models pytorch\n",
    "DeepLabV3Plus\n",
    "\n",
    "参考: https://www.kaggle.com/code/killa92/pytorch-miou-0-95-using-deeplabv3/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, torch, cv2, numpy as np, albumentations as A\n",
    "from PIL import Image; from matplotlib import pyplot as plt\n",
    "from glob import glob\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "class CustomSegmentationDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root, transformations = None):\n",
    "\n",
    "        self.im_paths = sorted(glob(f\"{root}/*/images/*.jpg\"))\n",
    "        self.gt_paths = sorted(glob(f\"{root}/*/masks/*.png\"))\n",
    "        self.transformations = transformations\n",
    "        self.n_cls = 5\n",
    "        \n",
    "        assert len(self.im_paths) == len(self.gt_paths)\n",
    "        \n",
    "    def __len__(self): return len(self.im_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        im, gt = self.get_im_gt(self.im_paths[idx], self.gt_paths[idx])\n",
    "        \n",
    "        if self.transformations: im, gt = self.apply_transformations(im, gt)\n",
    "        \n",
    "        return im, gt\n",
    "        \n",
    "    def get_im_gt(self, im_path, gt_path): return self.read_im(im_path), self.read_im(gt_path)\n",
    "\n",
    "    def read_im(self, path): return cv2.cvtColor(cv2.imread(path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    def apply_transformations(self, im, gt): transformed = self.transformations(image = im, mask = gt); return transformed[\"image\"], transformed[\"mask\"]\n",
    "\n",
    "def get_dls(root, transformations, bs, split = [0.9, 0.05, 0.05]):\n",
    "        \n",
    "    assert sum(split) == 1., \"Sum of the split must be exactly 1\"\n",
    "    \n",
    "    ds = CustomSegmentationDataset(root = root, transformations = transformations)\n",
    "    n_cls = ds.n_cls\n",
    "    \n",
    "    tr_len = int(len(ds) * split[0])\n",
    "    val_len = int(len(ds) * split[1])\n",
    "    test_len = len(ds) - (tr_len + val_len)\n",
    "    \n",
    "    # Data split\n",
    "    tr_ds, val_ds, test_ds = torch.utils.data.random_split(ds, [tr_len, val_len, test_len])\n",
    "        \n",
    "    print(f\"\\nThere are {len(tr_ds)} number of images in the train set\")\n",
    "    print(f\"There are {len(val_ds)} number of images in the validation set\")\n",
    "    print(f\"There are {len(test_ds)} number of images in the test set\\n\")\n",
    "    \n",
    "    # Get dataloaders\n",
    "    tr_dl  = DataLoader(dataset = tr_ds, batch_size = bs, shuffle = True, num_workers = 8)\n",
    "    val_dl = DataLoader(dataset = val_ds, batch_size = bs, shuffle = False, num_workers = 8)\n",
    "    test_dl = DataLoader(dataset = test_ds, batch_size = 1, shuffle = False, num_workers = 8)\n",
    "    \n",
    "    return tr_dl, val_dl, test_dl, n_cls\n",
    "\n",
    "root = \"/kaggle/input/semantic-segmentation-of-aerial-imagery/Semantic segmentation dataset\"\n",
    "mean, std, im_h, im_w = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225], 256, 256\n",
    "trans = A.Compose( [A.Resize(im_h, im_w), A.augmentations.transforms.Normalize(mean = mean, std = std), ToTensorV2(transpose_mask = True) ])\n",
    "tr_dl, val_dl, test_dl, n_cls = get_dls(root = root, transformations = trans, bs = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torchvision import transforms as tfs\n",
    "\n",
    "def tn_2_np(t): \n",
    "    invTrans = tfs.Compose([ tfs.Normalize(mean = [ 0., 0., 0. ], std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n",
    "                                tfs.Normalize(mean = [ -0.485, -0.456, -0.406 ], std = [ 1., 1., 1. ]) ])\n",
    "    \n",
    "    rgb = True if len(t) == 3 else False\n",
    "    \n",
    "    return (invTrans(t) * 255).detach().cpu().permute(1,2,0).numpy().astype(np.uint8) if rgb else (t*255).detach().cpu().numpy().astype(np.uint8)\n",
    "\n",
    "def plot(rows, cols, count, im, gt = None, title = \"Original Image\"):\n",
    "    \n",
    "    plt.subplot(rows, cols, count)\n",
    "    plt.imshow(tn_2_np(im.squeeze(0).float())) if gt else plt.imshow(tn_2_np(im.squeeze(0)))\n",
    "    plt.axis(\"off\"); plt.title(title)\n",
    "    \n",
    "    return count + 1\n",
    "\n",
    "def visualize(ds, n_ims):\n",
    "    \n",
    "    plt.figure(figsize = (25, 20))\n",
    "    rows = n_ims // 4; cols = n_ims // rows\n",
    "    count = 1\n",
    "    indices = [random.randint(0, len(ds) - 1) for _ in range(n_ims)]\n",
    "    \n",
    "    for idx, index in enumerate(indices):\n",
    "        \n",
    "        if count == n_ims + 1: break\n",
    "        im, gt = ds[index]\n",
    "        \n",
    "        # First Plot\n",
    "        count = plot(rows, cols, count, im = im)\n",
    "        \n",
    "        # Second Plot\n",
    "        count = plot(rows, cols, count, im = gt, gt = True)\n",
    "        \n",
    "visualize(tr_dl.dataset, n_ims = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install segmentation_models_pytorch\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "model = smp.DeepLabV3Plus(classes = n_cls)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = 3e-4)\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Metrics():\n",
    "    \n",
    "    def __init__(self, pred, gt, loss_fn, eps = 1e-10, n_cls = 2):\n",
    "        \n",
    "        self.pred, self.gt = torch.argmax(F.softmax(pred, dim=1), dim = 1), gt # (batch, width, height)\n",
    "        self.loss_fn, self.eps, self.n_cls, self.pred_ = loss_fn, eps, n_cls, pred\n",
    "        \n",
    "    def to_contiguous(self, inp): return inp.contiguous().view(-1) \n",
    "    \n",
    "    def PA(self):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            match = torch.eq(self.pred, self.gt).int()\n",
    "        \n",
    "        return float(match.sum()) / float(match.numel())\n",
    "\n",
    "    def mIoU(self):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.gt = torch.argmax(self.gt, dim = 1)\n",
    "            \n",
    "            pred, gt = self.to_contiguous(self.pred), self.to_contiguous(self.gt)\n",
    "\n",
    "            iou_per_class = []\n",
    "            \n",
    "            for c in range(self.n_cls):\n",
    "                \n",
    "                match_pred = pred == c\n",
    "                match_gt   = gt == c\n",
    "\n",
    "                if match_gt.long().sum().item() == 0: iou_per_class.append(np.nan)\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    intersect = torch.logical_and(match_pred, match_gt).sum().float().item()\n",
    "                    union = torch.logical_or(match_pred, match_gt).sum().float().item()\n",
    "\n",
    "                    iou = (intersect + self.eps) / (union + self.eps)\n",
    "                    iou_per_class.append(iou)\n",
    "                    \n",
    "            return np.nanmean(iou_per_class)\n",
    "    \n",
    "    def loss(self): return self.loss_fn(self.pred_, torch.argmax(self.gt, dim = 1))\n",
    "\n",
    "def tic_toc(start_time = None): return time.time() if start_time == None else time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, tr_dl, val_dl, loss_fn, opt, device, epochs, save_prefix, save_path = \"saved_models\"):\n",
    "    \n",
    "    tr_loss, tr_pa, tr_iou = [], [], []\n",
    "    val_loss, val_pa, val_iou = [], [], []\n",
    "    tr_len, val_len = len(tr_dl), len(val_dl)\n",
    "    best_loss, decrease, not_improve, early_stop_threshold = np.inf, 1, 0, 7\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    model.to(device)\n",
    "    train_start = tic_toc()\n",
    "    print(\"Start training process...\")\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        tic = tic_toc()\n",
    "        tr_loss_, tr_iou_, tr_pa_ = 0, 0, 0\n",
    "        \n",
    "        model.train()\n",
    "        print(f\"Epoch {epoch} train process is started...\")\n",
    "        for idx, batch in enumerate(tqdm(tr_dl)):\n",
    "            \n",
    "            ims, gts = batch\n",
    "            ims, gts = ims.to(device), gts.to(device)\n",
    "            \n",
    "            preds = model(ims)\n",
    "            \n",
    "            met = Metrics(preds, gts, loss_fn, n_cls = n_cls)\n",
    "            loss_ = met.loss()\n",
    "            \n",
    "            tr_iou_ += met.mIoU()\n",
    "            \n",
    "            tr_pa_ += met.PA()\n",
    "            tr_loss_ += loss_.item()\n",
    "            \n",
    "            loss_.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        \n",
    "        print(f\"Epoch {epoch} validation process is started...\")\n",
    "        model.eval()\n",
    "        val_loss_, val_iou_, val_pa_ = 0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in enumerate(tqdm(val_dl)):\n",
    "\n",
    "                ims, gts = batch\n",
    "                ims, gts = ims.to(device), gts.to(device)\n",
    "\n",
    "                preds = model(ims)\n",
    "\n",
    "                met = Metrics(preds, gts, loss_fn, n_cls = n_cls)\n",
    "\n",
    "                val_loss_ += met.loss().item()\n",
    "                val_iou_ += met.mIoU()\n",
    "                val_pa_ += met.PA()\n",
    "                \n",
    "\n",
    "        print(f\"Epoch {epoch} train process is completed.\")\n",
    "\n",
    "        tr_loss_ /= tr_len\n",
    "        tr_iou_ /= tr_len\n",
    "        tr_pa_ /= tr_len\n",
    "\n",
    "        val_loss_ /= val_len\n",
    "        val_iou_ /=  val_len\n",
    "        val_pa_ /=   val_len\n",
    "\n",
    "        print(\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "        print(f\"\\nEpoch {epoch} train process results: \\n\")\n",
    "        print(f\"Train Time         -> {tic_toc(tic):.3f} secs\")\n",
    "        print(f\"Train Loss         -> {tr_loss_:.3f}\")\n",
    "        print(f\"Train PA           -> {tr_pa_:.3f}\")\n",
    "        print(f\"Train IoU          -> {tr_iou_:.3f}\")\n",
    "        print(f\"Validation Loss    -> {val_loss_:.3f}\")\n",
    "        print(f\"Validation PA      -> {val_pa_:.3f}\")\n",
    "        print(f\"Validation IoU     -> {val_iou_:.3f}\\n\")\n",
    "\n",
    "        tr_loss.append(tr_loss_)\n",
    "        tr_iou.append(tr_iou_)\n",
    "        tr_pa.append(tr_pa_)\n",
    "\n",
    "        val_loss.append(val_loss_)\n",
    "        val_iou.append(val_iou_)\n",
    "        val_pa.append(val_pa_)\n",
    "        \n",
    "        if best_loss > (val_loss_):\n",
    "            print(f\"Loss decreased from {best_loss:.3f} to {val_loss_:.3f}!\")\n",
    "            best_loss = val_loss_\n",
    "            decrease += 1\n",
    "            if decrease % 2 == 0:\n",
    "                print(\"Saving the model with the best loss value...\")\n",
    "                torch.save(model, f\"{save_path}/{save_prefix}_best_model.pt\")\n",
    "\n",
    "        if val_loss_ > best_loss:\n",
    "\n",
    "            not_improve += 1\n",
    "            best_loss = val_loss_\n",
    "            print(f\"Loss did not decrease for {not_improve} epoch(s)!\")\n",
    "            if not_improve == early_stop_threshold:\n",
    "                print(f\"Stopping training process becuase loss value did not decrease for {early_stop_threshold} epochs!\")\n",
    "                break\n",
    "        print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n",
    "            \n",
    "    print(f\"Train process is completed in {(tic_toc(train_start)) / 60:.3f} minutes.\")\n",
    "    \n",
    "    return {\"tr_loss\": tr_loss, \"tr_iou\": tr_iou, \"tr_pa\": tr_pa,\n",
    "            \"val_loss\": val_loss, \"val_iou\": val_iou, \"val_pa\" : val_pa}\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "history = train(model = model, tr_dl = tr_dl, val_dl = val_dl,\n",
    "                 loss_fn = loss_fn, opt = optimizer, device = device,\n",
    "                 epochs = 50, save_prefix = \"aerial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plot():\n",
    "    \n",
    "    def __init__(self, res):\n",
    "        \n",
    "        self.res = res\n",
    "        \n",
    "        self.visualize(metric1 = \"tr_iou\", metric2 = \"val_iou\", label1 = \"Train IoU\", \n",
    "                  label2 = \"Validation IoU\", title = \"Mean Intersection Over Union Learning Curve\", ylabel = \"mIoU Score\")\n",
    "    \n",
    "        self.visualize(metric1 = \"tr_pa\", metric2 = \"val_pa\", label1 = \"Train PA\", \n",
    "                  label2 = \"Validation PA\", title = \"Pixel Accuracy Learning Curve\", ylabel = \"PA Score\")\n",
    "        \n",
    "        self.visualize(metric1 = \"tr_loss\", metric2 = \"val_loss\", label1 = \"Train Loss\", \n",
    "                  label2 = \"Validation Loss\", title = \"Loss Learning Curve\", ylabel = \"Loss Value\")\n",
    "        \n",
    "        \n",
    "    def plot(self, metric, label): plt.plot(self.res[metric], label = label)\n",
    "    \n",
    "    def decorate(self, ylabel, title): plt.title(title); plt.xlabel(\"Epochs\"); plt.ylabel(ylabel); plt.legend(); plt.show()\n",
    "    \n",
    "    def visualize(self, metric1, metric2, label1, label2, title, ylabel):\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        self.plot(metric1, label1); self.plot(metric2, label2)\n",
    "        self.decorate(ylabel, title)                \n",
    "        \n",
    "Plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(dl, model, device, n_ims = 15):\n",
    "    \n",
    "    cols = n_ims // 3; rows = n_ims // cols\n",
    "    \n",
    "    count = 1\n",
    "    ims, gts, preds = [], [], []\n",
    "    for idx, data in enumerate(dl):\n",
    "        im, gt = data\n",
    "\n",
    "        # Get predicted mask\n",
    "        with torch.no_grad(): pred = torch.argmax(model(im.to(device)), dim = 1)\n",
    "        ims.append(im); gts.append(gt); preds.append(pred)\n",
    "        \n",
    "    plt.figure(figsize = (25, 20))\n",
    "    for idx, (im, gt, pred) in enumerate(zip(ims, gts, preds)):\n",
    "        if idx == cols: break\n",
    "        \n",
    "        # First plot\n",
    "        count = plot(cols, rows, count, im)\n",
    "\n",
    "        # Second plot\n",
    "        count = plot(cols, rows, count, im = gt, gt = True, title = \"Ground Truth\")\n",
    "\n",
    "        # Third plot\n",
    "        count = plot(cols, rows, count, im = pred, title = \"Predicted Mask\")\n",
    "\n",
    "model = torch.load(\"/kaggle/working/saved_models/aerial_best_model.pt\")\n",
    "inference(test_dl, model = model, device = device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seg_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
