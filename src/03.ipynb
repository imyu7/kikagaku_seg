{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torchvision.models.segmentationを使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tifffile import TiffFile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vgg16_bn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from torchsummary import summary\n",
    "\n",
    "from torchvision.io.image import read_image\n",
    "from torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\n",
    "from torchvision.transforms.functional import to_pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../platelet_data'\n",
    "\n",
    "with TiffFile(os.path.join(data_dir, 'train-images.tif')) as tif:\n",
    "    train_img = tif.asarray()\n",
    "    \n",
    "with TiffFile(os.path.join(data_dir, 'train-labels.tif')) as tif:\n",
    "    train_label = tif.asarray()\n",
    "    \n",
    "with TiffFile(os.path.join(data_dir, 'eval-images.tif')) as tif:\n",
    "    eval_img = tif.asarray()\n",
    "\n",
    "with TiffFile(os.path.join(data_dir, 'eval-labels.tif')) as tif:\n",
    "    eval_label = tif.asarray()\n",
    "    \n",
    "with TiffFile(os.path.join(data_dir, 'test-images.tif')) as tif:\n",
    "    test_img = tif.asarray()\n",
    "    \n",
    "with TiffFile(os.path.join(data_dir, 'test-labels.tif')) as tif:\n",
    "    test_label = tif.asarray()\n",
    "\n",
    "M = max(np.max(train_img), np.max(eval_img), np.max(test_img))\n",
    "m = min(np.min(train_img), np.min(eval_img), np.min(test_img))\n",
    "train_img_norm = (train_img - m) / (M - m)\n",
    "eval_img_norm = (eval_img - m) / (M - m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        label = torch.from_numpy(label.astype(np.int64)).clone()\n",
    "        label = torch.nn.functional.one_hot(label.long(), num_classes=7)\n",
    "        label = label.to(torch.float32)\n",
    "        label = label.permute(2, 0, 1)\n",
    "        label = transforms.Resize((224, 224))(label)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# FCN-ResNet50 model用のtransform\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "    #                      std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# Creating the datasets\n",
    "train_dataset = CustomDataset(train_img_norm, train_label, transform=transform)\n",
    "eval_dataset = CustomDataset(eval_img_norm, eval_label, transform=transform)\n",
    "\n",
    "# DataLoader creation\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The parameter 'num_classes' expected value 21 but got 7 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/imyu/transx/kikagaku_seg/src/03.ipynb セル 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/03.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m weights \u001b[39m=\u001b[39m FCN_ResNet50_Weights\u001b[39m.\u001b[39mDEFAULT\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/03.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m fcn_resnet50(weights\u001b[39m=\u001b[39;49mweights, num_classes\u001b[39m=\u001b[39;49m\u001b[39m7\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/03.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mmps\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mbackends\u001b[39m.\u001b[39mmps\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/03.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# device = 'cpu'\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/seg/lib/python3.9/site-packages/torchvision/models/_utils.py:142\u001b[0m, in \u001b[0;36mkwonly_to_pos_or_kw.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    136\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUsing \u001b[39m\u001b[39m{\u001b[39;00msequence_to_str(\u001b[39mtuple\u001b[39m(keyword_only_kwargs\u001b[39m.\u001b[39mkeys()),\u001b[39m \u001b[39mseparate_last\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mand \u001b[39m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m as positional \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minstead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m     )\n\u001b[1;32m    140\u001b[0m     kwargs\u001b[39m.\u001b[39mupdate(keyword_only_kwargs)\n\u001b[0;32m--> 142\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/seg/lib/python3.9/site-packages/torchvision/models/_utils.py:228\u001b[0m, in \u001b[0;36mhandle_legacy_interface.<locals>.outer_wrapper.<locals>.inner_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[39mdel\u001b[39;00m kwargs[pretrained_param]\n\u001b[1;32m    226\u001b[0m     kwargs[weights_param] \u001b[39m=\u001b[39m default_weights_arg\n\u001b[0;32m--> 228\u001b[0m \u001b[39mreturn\u001b[39;00m builder(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/seg/lib/python3.9/site-packages/torchvision/models/segmentation/fcn.py:162\u001b[0m, in \u001b[0;36mfcn_resnet50\u001b[0;34m(weights, progress, num_classes, aux_loss, weights_backbone, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mif\u001b[39;00m weights \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     weights_backbone \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m     num_classes \u001b[39m=\u001b[39m _ovewrite_value_param(\u001b[39m\"\u001b[39;49m\u001b[39mnum_classes\u001b[39;49m\u001b[39m\"\u001b[39;49m, num_classes, \u001b[39mlen\u001b[39;49m(weights\u001b[39m.\u001b[39;49mmeta[\u001b[39m\"\u001b[39;49m\u001b[39mcategories\u001b[39;49m\u001b[39m\"\u001b[39;49m]))\n\u001b[1;32m    163\u001b[0m     aux_loss \u001b[39m=\u001b[39m _ovewrite_value_param(\u001b[39m\"\u001b[39m\u001b[39maux_loss\u001b[39m\u001b[39m\"\u001b[39m, aux_loss, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    164\u001b[0m \u001b[39melif\u001b[39;00m num_classes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/seg/lib/python3.9/site-packages/torchvision/models/_utils.py:246\u001b[0m, in \u001b[0;36m_ovewrite_value_param\u001b[0;34m(param, actual, expected)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[39mif\u001b[39;00m actual \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    245\u001b[0m     \u001b[39mif\u001b[39;00m actual \u001b[39m!=\u001b[39m expected:\n\u001b[0;32m--> 246\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe parameter \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m expected value \u001b[39m\u001b[39m{\u001b[39;00mexpected\u001b[39m}\u001b[39;00m\u001b[39m but got \u001b[39m\u001b[39m{\u001b[39;00mactual\u001b[39m}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    247\u001b[0m \u001b[39mreturn\u001b[39;00m expected\n",
      "\u001b[0;31mValueError\u001b[0m: The parameter 'num_classes' expected value 21 but got 7 instead."
     ]
    }
   ],
   "source": [
    "weights = FCN_ResNet50_Weights.DEFAULT\n",
    "model = fcn_resnet50(weights=weights, num_classes=7)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# device = 'cpu'\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分類クラス数を引数として受け取るのに、21以外対応していない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(next(model.parameters()).is_mps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/imyu/transx/kikagaku_seg/src/03.ipynb セル 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/03.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m summary(model, (\u001b[39m3\u001b[39;49m, \u001b[39m224\u001b[39;49m, \u001b[39m224\u001b[39;49m))\n",
      "File \u001b[0;32m~/anaconda3/envs/seg/lib/python3.9/site-packages/torchsummary/torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     68\u001b[0m model\u001b[39m.\u001b[39mapply(register_hook)\n\u001b[1;32m     70\u001b[0m \u001b[39m# make a forward pass\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m model(\u001b[39m*\u001b[39;49mx)\n\u001b[1;32m     74\u001b[0m \u001b[39m# remove these hooks\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m hooks:\n",
      "File \u001b[0;32m~/anaconda3/envs/seg/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/seg/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/seg/lib/python3.9/site-packages/torchvision/models/segmentation/_utils.py:23\u001b[0m, in \u001b[0;36m_SimpleSegmentationModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m input_shape \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:]\n\u001b[1;32m     22\u001b[0m \u001b[39m# contract: features is a dict of tensors\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(x)\n\u001b[1;32m     25\u001b[0m result \u001b[39m=\u001b[39m OrderedDict()\n\u001b[1;32m     26\u001b[0m x \u001b[39m=\u001b[39m features[\u001b[39m\"\u001b[39m\u001b[39mout\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/seg/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/seg/lib/python3.9/site-packages/torch/nn/modules/module.py:1581\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1580\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1581\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39;49m, args, result)\n\u001b[1;32m   1583\u001b[0m \u001b[39mif\u001b[39;00m hook_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1584\u001b[0m     result \u001b[39m=\u001b[39m hook_result\n",
      "File \u001b[0;32m~/anaconda3/envs/seg/lib/python3.9/site-packages/torchsummary/torchsummary.py:26\u001b[0m, in \u001b[0;36msummary.<locals>.register_hook.<locals>.hook\u001b[0;34m(module, input, output)\u001b[0m\n\u001b[1;32m     22\u001b[0m     summary[m_key][\u001b[39m\"\u001b[39m\u001b[39moutput_shape\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m [\n\u001b[1;32m     23\u001b[0m         [\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(o\u001b[39m.\u001b[39msize())[\u001b[39m1\u001b[39m:] \u001b[39mfor\u001b[39;00m o \u001b[39min\u001b[39;00m output\n\u001b[1;32m     24\u001b[0m     ]\n\u001b[1;32m     25\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     summary[m_key][\u001b[39m\"\u001b[39m\u001b[39moutput_shape\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(output\u001b[39m.\u001b[39;49msize())\n\u001b[1;32m     27\u001b[0m     summary[m_key][\u001b[39m\"\u001b[39m\u001b[39moutput_shape\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m batch_size\n\u001b[1;32m     29\u001b[0m params \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "# 内部のバグで、summaryが使えない\n",
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15183, 14960, 14124, ...,  8637,  8298,  9734],\n",
       "       [15088, 14882, 15025, ...,  7717,  7548,  8115],\n",
       "       [15085, 14925, 14265, ...,  8987,  9257,  8946],\n",
       "       ...,\n",
       "       [11024, 11678, 11054, ...,  5606,  6552,  6630],\n",
       "       [ 9074,  9459,  9957, ...,  5801,  7308,  7586],\n",
       "       [10355,  9609,  8369, ...,  6613,  5735,  5393]], dtype=int16)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = train_img[0]\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SemanticSegmentation(\n",
       "    resize_size=[520]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BILINEAR\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unexpected type <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/imyu/transx/kikagaku_seg/src/03.ipynb セル 8\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/03.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m preprocess \u001b[39m=\u001b[39m weights\u001b[39m.\u001b[39mtransforms()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/03.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m preprocess \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/03.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     transforms\u001b[39m.\u001b[39mResize((\u001b[39m224\u001b[39m,\u001b[39m224\u001b[39m)),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/03.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     transforms\u001b[39m.\u001b[39mToTensor(),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/03.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     transforms\u001b[39m.\u001b[39mNormalize(mean\u001b[39m=\u001b[39m[\u001b[39m0.485\u001b[39m, \u001b[39m0.456\u001b[39m, \u001b[39m0.406\u001b[39m], std\u001b[39m=\u001b[39m[\u001b[39m0.229\u001b[39m, \u001b[39m0.224\u001b[39m, \u001b[39m0.225\u001b[39m]),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/03.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m ])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/03.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m batch \u001b[39m=\u001b[39m preprocess(img)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/03.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m batch\n",
      "File \u001b[0;32m~/anaconda3/envs/seg/lib/python3.9/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/seg/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/seg/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/seg/lib/python3.9/site-packages/torchvision/transforms/transforms.py:361\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m    354\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mresize(img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mantialias)\n",
      "File \u001b[0;32m~/anaconda3/envs/seg/lib/python3.9/site-packages/torchvision/transforms/functional.py:476\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[39mif\u001b[39;00m max_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(size) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    471\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    472\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mmax_size should only be passed if size specifies the length of the smaller edge, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    473\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mi.e. size should be an int or a sequence of length 1 in torchscript mode.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    474\u001b[0m         )\n\u001b[0;32m--> 476\u001b[0m _, image_height, image_width \u001b[39m=\u001b[39m get_dimensions(img)\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(size, \u001b[39mint\u001b[39m):\n\u001b[1;32m    478\u001b[0m     size \u001b[39m=\u001b[39m [size]\n",
      "File \u001b[0;32m~/anaconda3/envs/seg/lib/python3.9/site-packages/torchvision/transforms/functional.py:78\u001b[0m, in \u001b[0;36mget_dimensions\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m     76\u001b[0m     \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39mget_dimensions(img)\n\u001b[0;32m---> 78\u001b[0m \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39;49mget_dimensions(img)\n",
      "File \u001b[0;32m~/anaconda3/envs/seg/lib/python3.9/site-packages/torchvision/transforms/_functional_pil.py:31\u001b[0m, in \u001b[0;36mget_dimensions\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     29\u001b[0m     width, height \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39msize\n\u001b[1;32m     30\u001b[0m     \u001b[39mreturn\u001b[39;00m [channels, height, width]\n\u001b[0;32m---> 31\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(img)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Unexpected type <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "preprocess = weights.transforms()\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "batch = preprocess(img)\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## segmentation_models_pytorchを使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tifffile import TiffFile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vgg16_bn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../platelet_data'\n",
    "\n",
    "with TiffFile(os.path.join(data_dir, 'train-images.tif')) as tif:\n",
    "    train_img = tif.asarray()\n",
    "    \n",
    "with TiffFile(os.path.join(data_dir, 'train-labels.tif')) as tif:\n",
    "    train_label = tif.asarray()\n",
    "    \n",
    "with TiffFile(os.path.join(data_dir, 'eval-images.tif')) as tif:\n",
    "    eval_img = tif.asarray()\n",
    "    \n",
    "with TiffFile(os.path.join(data_dir, 'eval-labels.tif')) as tif:\n",
    "    eval_label = tif.asarray()\n",
    "    \n",
    "with TiffFile(os.path.join(data_dir, 'test-images.tif')) as tif:\n",
    "    test_img = tif.asarray()\n",
    "    \n",
    "with TiffFile(os.path.join(data_dir, 'test-labels.tif')) as tif:\n",
    "    test_label = tif.asarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = max(np.max(train_img), np.max(eval_img), np.max(test_img))\n",
    "m = min(np.min(train_img), np.min(eval_img), np.min(test_img))\n",
    "train_img_norm = (train_img - m) / (M - m)\n",
    "eval_img_norm = (eval_img - m) / (M - m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        label = torch.from_numpy(label.astype(np.int64)).clone()\n",
    "        label = torch.nn.functional.one_hot(label.long(), num_classes=7)\n",
    "        label = label.to(torch.float32)\n",
    "        label = label.permute(2, 0, 1)\n",
    "        # label = transforms.Resize((224, 224))(label)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            image = image.to(torch.float32)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# FCN-ResNet50 model用のtransform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Resize((224, 224)), \n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "    #                      std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# Creating the datasets\n",
    "train_dataset = CustomDataset(train_img_norm, train_label, transform=transform)\n",
    "eval_dataset = CustomDataset(eval_img_norm, eval_label, transform=transform)\n",
    "\n",
    "# DataLoader creation\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデル1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# segmentation_models_pytorchモジュールの読み込み\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "# from pytorch_lightning.metrics.functional import accuracy\n",
    "\n",
    "\n",
    "# UNetの構築\n",
    "\n",
    "class Net_resnet18(pl.LightningModule):\n",
    "    def __init__(self, in_channels=1, n_classes=7):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.unet = smp.Unet(\"resnet18\", in_channels=in_channels, classes=n_classes, encoder_weights=\"imagenet\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.unet(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, t = batch\n",
    "        y = self(x)\n",
    "        # loss = F.cross_entropy(y, t)\n",
    "        loss = F.binary_cross_entropy_with_logits(y, t)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True)\n",
    "        # self.log(\"train_acc\", accuracy(y.softmax(dim=1), t), on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, t = batch\n",
    "        y = self(x)\n",
    "        # loss = F.cross_entropy(y, t)\n",
    "        loss = F.binary_cross_entropy_with_logits(y, t)\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True)\n",
    "        # self.log(\"val_acc\", accuracy(y.softmax(dim=1), t), on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "0 | unet | Unet | 14.3 M\n",
      "------------------------------\n",
      "14.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "14.3 M    Total params\n",
      "57.291    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imyu/anaconda3/envs/seg_new/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/imyu/anaconda3/envs/seg_new/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (13) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 13/13 [00:21<00:00,  0.61it/s, v_num=7]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 13/13 [00:21<00:00,  0.60it/s, v_num=7]\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(0)\n",
    "net_resnet18 = Net_resnet18()\n",
    "trainer = pl.Trainer(max_epochs=3, deterministic=False)\n",
    "trainer.fit(net_resnet18, train_loader, eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (MPSFloatType) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/imyu/transx/kikagaku_seg/src/03.ipynb セル 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/03.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/03.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/03.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m outputs \u001b[39m=\u001b[39m net_resnet18(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/03.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(jaccard_index(outputs, labels, num_classes\u001b[39m=\u001b[39m\u001b[39m7\u001b[39m, task\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/03.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m scores\u001b[39m.\u001b[39mappend(jaccard_index(outputs, labels, num_classes\u001b[39m=\u001b[39m\u001b[39m7\u001b[39m, task\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/anaconda3/envs/seg_new/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/seg_new/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/imyu/transx/kikagaku_seg/src/03.ipynb セル 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/03.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/03.ipynb#X20sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munet(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/seg_new/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/seg_new/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/seg_new/lib/python3.11/site-packages/segmentation_models_pytorch/base/model.py:29\u001b[0m, in \u001b[0;36mSegmentationModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_input_shape(x)\n\u001b[0;32m---> 29\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[1;32m     30\u001b[0m decoder_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(\u001b[39m*\u001b[39mfeatures)\n\u001b[1;32m     32\u001b[0m masks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msegmentation_head(decoder_output)\n",
      "File \u001b[0;32m~/anaconda3/envs/seg_new/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/seg_new/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/seg_new/lib/python3.11/site-packages/segmentation_models_pytorch/encoders/resnet.py:62\u001b[0m, in \u001b[0;36mResNetEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     60\u001b[0m features \u001b[39m=\u001b[39m []\n\u001b[1;32m     61\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_depth \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m---> 62\u001b[0m     x \u001b[39m=\u001b[39m stages[i](x)\n\u001b[1;32m     63\u001b[0m     features\u001b[39m.\u001b[39mappend(x)\n\u001b[1;32m     65\u001b[0m \u001b[39mreturn\u001b[39;00m features\n",
      "File \u001b[0;32m~/anaconda3/envs/seg_new/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/seg_new/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/seg_new/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/seg_new/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/seg_new/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/seg_new/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/seg_new/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (MPSFloatType) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "from torchmetrics.functional import jaccard_index\n",
    "\n",
    "scores = []\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "\n",
    "net_resnet18.eval()\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(eval_loader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.float().to(device)\n",
    "        labels = labels.float().to(device)\n",
    "        outputs = net_resnet18(inputs)\n",
    "        print(jaccard_index(outputs, labels, num_classes=7, task=\"multiclass\"))\n",
    "        scores.append(jaccard_index(outputs, labels, num_classes=7, task=\"multiclass\").to(\"cpu\").numpy())\n",
    "        # print(jaccard_index(labels, labels, num_classes=7, task=\"multiclass\"))\n",
    "        \n",
    "print(np.mean(scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデル2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imyu/anaconda3/envs/seg_new/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# パラメーター\n",
    "ENCODER = \"efficientnet-b4\"\n",
    "ENCODER_WEIGHTS = \"imagenet\"\n",
    "ACTIVATION = \"softmax2d\"\n",
    "CLASS_NUM = 7 # segmentationの正解ラベル数\n",
    "\n",
    "# モデル定義\n",
    "model = smp.UnetPlusPlus(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=CLASS_NUM, \n",
    "    activation=ACTIVATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "iou_score() missing 4 required positional arguments: 'tp', 'fp', 'fn', and 'tn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/imyu/transx/kikagaku_seg/src/03.ipynb セル 23\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/03.ipynb#X32sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m loss \u001b[39m=\u001b[39m smp\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mJaccardLoss(mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/03.ipynb#X32sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# metrics = [smp.utils.metrics.IoU(threshold=0.5)]\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/03.ipynb#X32sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m metrics \u001b[39m=\u001b[39m [smp\u001b[39m.\u001b[39;49mmetrics\u001b[39m.\u001b[39;49miou_score()]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/03.ipynb#X32sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam([\u001b[39mdict\u001b[39m(params\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.0001\u001b[39m)])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/03.ipynb#X32sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m train_epoch \u001b[39m=\u001b[39m smp\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mtrain\u001b[39m.\u001b[39mTrainEpoch(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/03.ipynb#X32sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     model, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/03.ipynb#X32sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     loss\u001b[39m=\u001b[39mloss, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/03.ipynb#X32sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/imyu/transx/kikagaku_seg/src/03.ipynb#X32sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: iou_score() missing 4 required positional arguments: 'tp', 'fp', 'fn', and 'tn'"
     ]
    }
   ],
   "source": [
    "# 学習時の設定\n",
    "DEVICE = \"cuda\"\n",
    "BATCH_SIZE = 1\n",
    "# loss = smp.utils.losses.DiceLoss()\n",
    "loss = smp.losses.JaccardLoss(mode='multiclass')\n",
    "# metrics = [smp.utils.metrics.IoU(threshold=0.5)]\n",
    "metrics = [smp.metrics.iou_score()]\n",
    "optimizer = torch.optim.Adam([dict(params=model.parameters(), lr=0.0001)])\n",
    "\n",
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
